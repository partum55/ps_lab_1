# ps_lab_1

## 1. train_multinomial_nb
Навчає Мультиноміальний Наївний Баєсівський класифікатор.
  
### --- Крок 1: Ініціалізація ---
  
Конвертуємо мітки в символьний тип для надійності.

Знаходимо унікальні класи (напр., "ham", "spam") та сортуємо їх.

Кількість документів у навчальній вибірці.

### --- Крок 2: Обчислення апріорних імовірностей P(class) ---
  
Рахуємо, скільки разів кожен клас з'являється в даних.

Обчислюємо апріорну ймовірність як частку кожного класу в загальній кількості.

Беремо логарифм від імовірностей, щоб уникнути помилок при перемноженні
дуже малих чисел і для спрощення подальших обчислень (додавання замість множення).
  
### --- Крок 3: Обчислення імовірностей слів P(word | class) ---
Створюємо матрицю для зберігання кількості кожного слова для кожного класу.
Рядки - це класи ("ham", "spam"), стовпці - це слова зі словника.

Для кожного класу ("ham", а потім "spam")...
...знаходимо всі документи (рядки), що належать до цього класу.
...підсумовуємо кількість кожного слова (по стовпцях) у цих документах.

Рахуємо загальну кількість всіх слів у кожному класі.
  
  
### --- Крок 4: Застосування згладжування Лапласа та логарифмування ---
  
Чисельник: (кількість слова 'w' у класі 'c') + alpha
Знаменник: (загальна кількість слів у класі 'c') + alpha * (розмір словника)

Обчислюємо логарифм імовірностей. Транспонуємо, щоб розмірність відповідала формату моделі: рядки - класи, стовпці - слова.

### --- Крок 5: Формування фінального об'єкта моделі ---
  
Зберігаємо всі обчислені параметри в одному списку. Це і є наша навчена модель.

## 2. predict_multinomial_nb
Робить прогнози для нових даних за допомогою навченої моделі.

### --- Крок 1: Підготовка матриці нових даних ---
  
Отримуємо словник, на якому навчалася модель.

Створюємо нову матрицю з нулів, яка має таку саму кількість стовпців (слів), як і навчальні дані. Це потрібно, щоб структура матриць збігалася.

Знаходимо спільні слова між словником моделі та новими даними.

Копіюємо лічильники для спільних слів у вирівняну матрицю. Слова з нових даних, яких не було в словнику моделі, ігноруються.

### --- Крок 2: Розрахунок оцінок (scores) для кожного класу ---
  
Ключовий крок: множимо матрицю нових документів на транспоновану матрицю логарифмічних імовірностей. Це ефективний спосіб розрахувати суму log(P(word|class)) для кожного документа і кожного класу.
  
Додаємо логарифм апріорної імовірності до оцінки кожного класу.
Це фіналізує формулу Баєса в логарифмічному просторі.
  
### --- Крок 3: Визначення переможного класу ---
  
Для кожного документа (рядка в матриці scores_mat) знаходимо, який клас (стовпець) має найвищу оцінку.

Повертаємо назви класів, що перемогли.
