---
editor_options:
  markdown:
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Probability and Statistics

# Lab Assignment 1: Naive Bayes Classifier

## Work breakdown

-   *Nazar Mykhailyshchuk*:
-   *Nikita Lenyk*:
-   *Anastasiya Shopska*:

## Package Installation

```{r eval=FALSE}
options(repos = c(CRAN = "https://cran.rstudio.com/"))
install.packages(c("readr", "stringi", "tokenizers", "SnowballC", "textstem", "tidyverse", "tidytext", "ggplot2", "wordcloud", "RColorBrewer"))
```

## Setup
```{r message=FALSE, warning=FALSE}
library(readr)
library(textstem)
library(stringi)
library(tokenizers)
library(SnowballC)
library(tidyverse)
library(tidytext)
library(ggplot2)
library(wordcloud)
library(RColorBrewer)
```

## Helper functions for text preprocessing

```{r}
# Remove stopwords from a vector of tokens (preserving order and duplicates)
remove_stopwords_from_tokens <- function(tokens, stopwords_list) {
  tokens[!tokens %in% stopwords_list]
}

# Apply stemming to tokens if they exist
apply_stemming_to_tokens <- function(tokens) {
  if (length(tokens) > 0) {
    SnowballC::wordStem(tokens, language = "english")
  } else {
    tokens
  }
}

# Apply lemmatization to tokens if they exist
apply_lemmatization_to_tokens <- function(tokens) {
  if (length(tokens) > 0) {
    textstem::lemmatize_words(tokens)
  } else {
    tokens
  }
}

# Generate word n-grams from tokens
generate_ngrams_from_tokens <- function(tokens, n) {
  if (length(tokens) >= n) {
    sapply(seq_len(length(tokens) - n + 1), function(i) {
      paste(tokens[i:(i + n - 1)], collapse = "_")
    })
  } else {
    character(0)
  }
}
```

## Text Preprocessing Pipeline
```{r}
#' Build Text Preprocessing Pipeline for SMS Spam Classification
#'
#' @param texts Character vector of text messages to preprocess
#' @param do_stem Apply stemming (default: FALSE)
#' @param do_lemma Apply lemmatization (default: FALSE)
#' @param remove_stopwords Remove stop words (default: TRUE)
#' @param stopwords_list Custom stop words list (default: NULL)
#' @param ngram N-gram size: 1=unigrams, 2=bigrams, etc. (default: 1)
#' @param tokenize_special Tokenize emails/phones/URLs as special tokens (default: TRUE)
#' @param vocabulary Pre-defined vocabulary (for test data). If NULL, builds vocabulary from texts (for train data)
#'
#' @return List with dtm (matrix), vocabulary, and preprocessing_info
build_preprocessing_pipeline <- function(texts,
                                         do_stem = FALSE,
                                         do_lemma = FALSE,
                                         remove_stopwords = TRUE,
                                         stopwords_list = NULL,
                                         ngram = 1,
                                         tokenize_special = TRUE,
                                         vocabulary = NULL) {
  if (!is.character(texts) || length(texts) == 0) {
    stop("texts must be a non-empty character vector")
  }

  if (!is.numeric(ngram) || length(ngram) != 1 || ngram < 1 || ngram != as.integer(ngram)) {
    stop("ngram must be a single positive integer")
  }

  if (do_stem && do_lemma) {
    warning("Both stemming and lemmatization enabled. Using lemmatization only (recommended).")
    do_stem <- FALSE
  }

  preprocessing_info <- list(
    do_stem = do_stem,
    do_lemma = do_lemma,
    remove_stopwords = remove_stopwords,
    custom_stopwords = !is.null(stopwords_list),
    ngram = ngram,
    tokenize_special = tokenize_special,
    original_length = length(texts)
  )

  # Step 1: Text cleaning and normalization
  clean_texts <- stringi::stri_trans_tolower(texts)

  if (tokenize_special) {
    # Replace special patterns with tokens
    clean_texts <- stringi::stri_replace_all_regex(clean_texts, "\\b[a-z0-9._%+-]+@[a-z0-9.-]+\\.[a-z]{2,}\\b", " EMAIL ")
    clean_texts <- stringi::stri_replace_all_regex(clean_texts, "\\bhttps?://\\S+\\b", " URL ")
    clean_texts <- stringi::stri_replace_all_regex(clean_texts, "\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b", " PHONE ")
    clean_texts <- stringi::stri_replace_all_regex(clean_texts, "\\$\\d+(?:\\.\\d{2})?", " MONEY ")
    clean_texts <- stringi::stri_replace_all_regex(clean_texts, "\\b\\d{5,}\\b", " LONGNUM ")
    clean_texts <- stringi::stri_replace_all_regex(clean_texts, "[^a-z\\s]", " ")
    clean_texts <- stringi::stri_replace_all_regex(clean_texts, "\\s+", " ")
    clean_texts <- stringi::stri_trim_both(clean_texts)
  } else {
    # Standard cleaning
    clean_texts <- stringi::stri_replace_all_regex(clean_texts, "[^a-z\\s]", " ")
    clean_texts <- stringi::stri_replace_all_regex(clean_texts, "\\s+", " ")
    clean_texts <- stringi::stri_trim_both(clean_texts)
  }

  # Step 2: Tokenization
  tokens_list <- tokenizers::tokenize_words(clean_texts, lowercase = FALSE, strip_punct = FALSE)

  # Step 3: Stop words removal
  if (remove_stopwords) {
    if (is.null(stopwords_list)) {
      stopwords_list <- readLines("data/stop_words.txt", warn = FALSE)
    }

    tokens_list <- lapply(tokens_list, remove_stopwords_from_tokens, stopwords_list)
  }

  # Step 4: Stemming
  if (do_stem) {
    if (!requireNamespace("SnowballC", quietly = TRUE)) {
      warning("SnowballC package not available. Skipping stemming.")
    } else {
      tokens_list <- lapply(tokens_list, apply_stemming_to_tokens)
    }
  }

  # Step 5: Lemmatization
  if (do_lemma) {
    if (!requireNamespace("textstem", quietly = TRUE)) {
      warning("textstem package not available. Skipping lemmatization.")
    } else {
      tokens_list <- lapply(tokens_list, apply_lemmatization_to_tokens)
    }
  }
  # Step 6: N-gram generation
  if (ngram == 1) {
    ngram_tokens <- tokens_list
  } else {
    ngram_tokens <- lapply(tokens_list, generate_ngrams_from_tokens, ngram)
  }

  # Step 7: Build vocabulary and create DTM
  # Count term frequencies across all documents
  all_terms <- unlist(ngram_tokens)

  # Use provided vocabulary (for test data) or build from current texts (for train data)
  if (is.null(vocabulary)) {
    term_freq <- table(all_terms)
    vocabulary <- sort(names(term_freq))
  }

  # Create Document-Term Matrix using base R
  # Initialize empty matrix
  dtm <- matrix(0,
    nrow = length(ngram_tokens),
    ncol = length(vocabulary),
    dimnames = list(
      docs = paste0("doc", seq_along(ngram_tokens)),
      terms = vocabulary
    )
  )

  # Fill the matrix
  for (doc_idx in seq_along(ngram_tokens)) {
    doc_terms <- ngram_tokens[[doc_idx]]

    # Keep only terms that are in vocabulary
    doc_terms_in_vocab <- doc_terms[doc_terms %in% vocabulary]

    if (length(doc_terms_in_vocab) > 0) {
      # Count term frequencies for this document
      doc_term_counts <- table(doc_terms_in_vocab)

      # Update the matrix for this document
      for (term in names(doc_term_counts)) {
        dtm[doc_idx, term] <- doc_term_counts[[term]]
      }
    }
  }

  # Update preprocessing info with final statistics
  preprocessing_info$final_vocab_size <- length(vocabulary)
  preprocessing_info$final_doc_count <- nrow(dtm)
  preprocessing_info$dtm_sparsity <- 1 - (sum(dtm != 0) / (nrow(dtm) * ncol(dtm)))

  return(list(
    dtm = dtm,
    vocabulary = vocabulary,
    preprocessing_info = preprocessing_info
  ))
}
```

## Example Usage

```{r}
train_data <- read_csv("data/4-spam/train.csv", show_col_types = FALSE)

train_preprocessed <- build_preprocessing_pipeline(
  texts = train_data$Message,
  do_stem = FALSE,
  do_lemma = TRUE,
  remove_stopwords = TRUE,
  stopwords_list = NULL,
  ngram = 1, # unigrams, bigrams=2, trigrams=3
  tokenize_special = TRUE,
  vocabulary = NULL
)

dtm <- train_preprocessed$dtm
rownames(dtm) <- paste0("train_", seq_len(nrow(dtm)))


term_freq <- colSums(dtm)
term_freq_sorted <- sort(term_freq, decreasing = TRUE)

top_terms_table <- data.frame(
  term = names(term_freq_sorted),
  freq = as.integer(term_freq_sorted)
)[1:10, ]



top_terms <- top_terms_table$term
dtm_top <- dtm[, top_terms]

dtm_top[1:5, ]
```
```{r}
top_terms_table
```


## Data Visualization

### Analysis of Most Common Words in SPAM and HAM Messages

```{r}
# Split data into spam and ham messages
spam_messages <- train_data$Message[train_data$Category == "spam"]
ham_messages <- train_data$Message[train_data$Category == "ham"]

# Process spam messages
spam_preprocessed <- build_preprocessing_pipeline(
  texts = spam_messages,
  do_lemma = TRUE,
  remove_stopwords = TRUE,
  ngram = 1,
  tokenize_special = TRUE,
  vocabulary = NULL
)

# Process ham messages
ham_preprocessed <- build_preprocessing_pipeline(
  texts = ham_messages,
  do_lemma = TRUE,
  remove_stopwords = TRUE,
  ngram = 1,
  tokenize_special = TRUE,
  vocabulary = NULL
)

# Get top words for each class
spam_word_freq <- colSums(spam_preprocessed$dtm)
ham_word_freq <- colSums(ham_preprocessed$dtm)

# Top 20 words for spam and ham
top_spam <- head(sort(spam_word_freq, decreasing = TRUE), 20)
top_ham <- head(sort(ham_word_freq, decreasing = TRUE), 20)
```

```{r}
head(top_spam, 10)
```

```{r}
head(top_ham, 10)
```

### 1. Histogram of Most Common Words

```{r}
# Create data frame for visualization
spam_df <- data.frame(
  word = names(top_spam),
  freq = as.numeric(top_spam),
  type = "SPAM",
  stringsAsFactors = FALSE
)

ham_df <- data.frame(
  word = names(top_ham),
  freq = as.numeric(top_ham),
  type = "HAM",
  stringsAsFactors = FALSE
)

# Combine data
words_combined <- rbind(spam_df, ham_df)

# Create histogram
ggplot(words_combined, aes(x = reorder(word, freq), y = freq, fill = type)) +
  geom_col(show.legend = TRUE) +
  facet_wrap(~type, scales = "free", ncol = 1) +
  coord_flip() +
  labs(
    title = "Most Common Words in SPAM and HAM Messages",
    subtitle = "Top 20 words after preprocessing pipeline",
    x = "Words",
    y = "Frequency",
    fill = "Message Type"
  ) +
  theme_minimal() +
  scale_fill_manual(values = c("SPAM" = "#FF6B6B", "HAM" = "#4ECDC4"))
```

### 2. Side-by-side Comparison

```{r}
# Take top 10 for better readability
top_spam_10 <- head(top_spam, 10)
top_ham_10 <- head(top_ham, 10)

# Create data frame
comparison_df <- data.frame(
  word = c(names(top_spam_10), names(top_ham_10)),
  freq = c(as.numeric(top_spam_10), as.numeric(top_ham_10)),
  type = c(rep("SPAM", 10), rep("HAM", 10))
)

ggplot(comparison_df, aes(x = reorder(word, freq), y = freq, fill = type)) +
  geom_col(position = "dodge", alpha = 0.8) +
  coord_flip() +
  labs(
    title = "Top 10 Words Comparison: SPAM vs HAM",
    x = "Words",
    y = "Frequency",
    fill = "Type"
  ) +
  theme_minimal() +
  scale_fill_manual(values = c("SPAM" = "#FF6B6B", "HAM" = "#4ECDC4")) +
  theme(legend.position = "bottom")
```

### 3. Analysis of Unique Words

```{r}
# Find words that exist only in spam or only in ham
spam_only_words <- setdiff(names(spam_word_freq), names(ham_word_freq))
ham_only_words <- setdiff(names(ham_word_freq), names(spam_word_freq))

# Create summary data frame
unique_words_summary <- data.frame(
  Category = c("SPAM only words", "HAM only words"),
  Count = c(length(spam_only_words), length(ham_only_words))
)
unique_words_summary
```

```{r}
# Top unique words in SPAM
if (length(spam_only_words) > 0) {
  spam_unique_freq <- spam_word_freq[spam_only_words]
  top_spam_unique <- head(sort(spam_unique_freq, decreasing = TRUE), 10)
  data.frame(
    Word = names(top_spam_unique),
    Frequency = as.numeric(top_spam_unique),
    row.names = NULL
  )
}
```

```{r}
# Top unique words in HAM
if (length(ham_only_words) > 0) {
  ham_unique_freq <- ham_word_freq[ham_only_words]
  top_ham_unique <- head(sort(ham_unique_freq, decreasing = TRUE), 10)
  data.frame(
    Word = names(top_ham_unique),
    Frequency = as.numeric(top_ham_unique),
    row.names = NULL
  )
}
```

### 4. Dataset Statistics

```{r}
# Dataset overview
dataset_stats <- data.frame(
  Metric = c("Total messages", "SPAM messages", "HAM messages", "SPAM percentage"),
  Value = c(
    nrow(train_data),
    sum(train_data$Category == "spam"),
    sum(train_data$Category == "ham"),
    paste0(round(100 * sum(train_data$Category == "spam") / nrow(train_data), 2), "%")
  )
)
dataset_stats
```

```{r}
# Vocabulary statistics
vocab_stats <- data.frame(
  Metric = c("Unique words in SPAM", "Unique words in HAM", "Total vocabulary size"),
  Count = c(
    length(spam_word_freq),
    length(ham_word_freq),
    length(union(names(spam_word_freq), names(ham_word_freq)))
  )
)
vocab_stats
```

## Classifier implementation

```{r}
naiveBayes <- setRefClass("naiveBayes",
  fields = list(
    model = "list",
    vocabulary = "character",
    preprocessing_params = "list"
  ),
  methods = list(
    # prepare your training data as X - bag of words for each of your
    # messages and corresponding label for the message encoded as 0 or 1
    fit = function(X, y, laplace = 1, do_stem = FALSE, do_lemma = TRUE, remove_stopwords = TRUE, ngram = 1, tokenize_special = TRUE) {
      train_preprocessed <- build_preprocessing_pipeline(
        texts = X,
        do_stem = do_stem,
        do_lemma = do_lemma,
        remove_stopwords = remove_stopwords,
        stopwords_list = NULL,
        ngram = ngram,
        tokenize_special = tokenize_special,
        vocabulary = NULL
      )

      dtm <- train_preprocessed$dtm
      vocabulary <<- train_preprocessed$vocabulary
      # Store preprocessing parameters from pipeline
      preprocessing_params <<- train_preprocessed$preprocessing_info

      # Train multinomial Naive Bayes
      labels <- as.character(y)
      classes <- sort(unique(labels))
      n_docs <- nrow(dtm)
      n_terms <- ncol(dtm)

      # calculate priority probabilities
      class_counts <- table(labels)
      class_priors <- class_counts / n_docs
      log_priors <- log(class_priors) # avoid overflow

      # word probability
      term_counts_by_class <- matrix(0,
        nrow = length(classes), ncol = n_terms,
        dimnames = list(classes, colnames(dtm))
      )
      for (cls in classes) {
        rows_in_class <- which(labels == cls)
        term_counts_by_class[cls, ] <- colSums(dtm[rows_in_class, , drop = FALSE])
      }

      class_total_terms <- rowSums(term_counts_by_class)

      # laplace and log
      numerator <- term_counts_by_class + laplace
      denominator <- class_total_terms + laplace * n_terms

      log_likelihoods <- log(t(numerator / denominator)) # log this

      # Store model
      model <<- list(
        classes = classes,
        log_priors = log_priors,
        log_likelihoods = log_likelihoods,
        vocabulary = colnames(dtm)
      )
    },
    predict = function(message) {
      if (is.null(model) || length(model) == 0) {
        stop("Model not trained. Please call fit() first.")
      }
      test_preprocessed <- build_preprocessing_pipeline(
        texts = message,
        do_stem = preprocessing_params$do_stem,
        do_lemma = preprocessing_params$do_lemma,
        remove_stopwords = preprocessing_params$remove_stopwords,
        stopwords_list = NULL,
        ngram = preprocessing_params$ngram,
        tokenize_special = preprocessing_params$tokenize_special,
        vocabulary = vocabulary
      )

      dtm_new <- test_preprocessed$dtm

      vocab <- model$vocabulary
      dtm_aligned <- matrix(0,
        nrow = nrow(dtm_new), ncol = length(vocab),
        dimnames = list(rownames(dtm_new), vocab)
      )
      common_cols <- intersect(colnames(dtm_new), vocab)
      if (length(common_cols) > 0) {
        dtm_aligned[, common_cols] <- dtm_new[, common_cols]
      }

      # Calculate scores
      # dtm_aligned: n_docs x n_vocab, log_likelihoods: n_vocab x n_classes
      scores_mat <- dtm_aligned %*% model$log_likelihoods
      scores_mat <- sweep(scores_mat, 2, model$log_priors, "+")

      # Get winning class
      best_class_idx <- max.col(scores_mat, ties.method = "first")
      predictions <- model$classes[best_class_idx]
      return(predictions)
    },
    score = function(X_test, y_test) {
      # TODO
    }
  )
)

model <- naiveBayes()
```

## Example Usage

```{r}
train_data <- read_csv("data/4-spam/train.csv", show_col_types = FALSE)
model <- naiveBayes()
model$fit(X = train_data$Message, y = train_data$Category)

test_message <- "Free money!!! Click here to claim your prize."
prediction <- model$predict(test_message)
print(paste("Prediction:", prediction))

test_data <- read_csv("data/4-spam/test.csv", show_col_types = FALSE)
test_predictions <- sapply(test_data$Message, function(msg) model$predict(msg))
print(paste("First 5 predictions:", paste(test_predictions[1:5], collapse = ", ")))
```
